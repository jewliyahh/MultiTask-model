{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Criteria:\n",
    "- Quality, depth, and clarity of explanations. Itâ€™s essential for us to be able to understand\n",
    "your thought process\n",
    "- Clarity and structure of the code.\n",
    "- Ability for us to easily run your code and replicate your results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implement a Sentence Transformer Model\n",
    "- Implement a sentence transformer model using any deep learning framework of your\n",
    "choice. This model should be able to encode input sentences into fixed-length\n",
    "embeddings.\n",
    "- Test your implementation with a few sample sentences and showcase the obtained\n",
    "embeddings.\n",
    "- Discuss any choices you had to make regarding the model architecture outside of the\n",
    "transformer backbone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained transformer model from Hugging Face\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences: actual fetch reviews hehe\n",
    "sentences = [\n",
    "    \"It CAN take a while for the points to collect\",\n",
    "    \"Fetch Rewards is a game-changer for shoppers!\",\n",
    "    \"I guess over the years since more people started using Fetch, they make it hard to get gift cards.\"\n",
    "]\n",
    "\n",
    "# Tokenize the input sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for sentence 1: 384\n",
      "\n",
      "Embedding for sentence 2: 384\n",
      "\n",
      "Embedding for sentence 3: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state is a tensor of size [batch_size, sequence_length, hidden_size]\n",
    "# mean-pool over the sequence length to create fixed-length sentence embeddings --> end shape (batch_size, hidden_size)\n",
    "sentence_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# sanity check that they are the same lenght\n",
    "for i, emb in enumerate(sentence_embeddings):\n",
    "    print(f\"Embedding for sentence {i+1}: {len(emb)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for sentence 1: tensor([-1.4879e-01, -3.2041e-01, -5.4127e-02,  1.6908e-01,  1.8378e-01,\n",
      "         2.3295e-01,  3.3648e-03, -2.6190e-01, -4.2113e-02,  7.5660e-03,\n",
      "         2.1651e-02, -5.0472e-02, -3.9413e-02,  8.1617e-02,  1.3642e-01,\n",
      "         1.3107e-01, -5.5834e-03, -4.0379e-01,  1.1190e-01, -8.1360e-02,\n",
      "        -1.9157e-01, -1.7363e-01, -3.1898e-01,  6.7866e-02,  2.7781e-01,\n",
      "         1.9570e-01, -2.9567e-02,  1.5387e-01,  1.3636e-01, -2.1507e-01,\n",
      "        -2.5315e-01, -6.5974e-03,  8.2477e-02,  2.0189e-01, -9.6518e-02,\n",
      "        -7.1657e-02,  2.9166e-03,  1.5178e-01,  3.9197e-02, -2.0562e-01,\n",
      "         2.4474e-01, -3.0083e-02,  6.2201e-02,  4.2204e-01,  7.8106e-02,\n",
      "         1.7176e-01,  2.3724e-01, -2.0900e-01,  2.4517e-01,  1.3527e-01,\n",
      "         1.3883e-01,  3.7395e-01, -4.1784e-02,  2.2985e-02, -1.2272e-01,\n",
      "         1.0782e-01,  4.2287e-02,  2.4941e-02, -2.1965e-01, -1.4539e-01,\n",
      "         7.3403e-02, -5.3891e-02, -2.7122e+00, -1.7283e-01,  1.0310e-01,\n",
      "        -3.1633e-01,  6.1088e-02, -1.6372e-01,  6.8121e-02, -2.9211e-01,\n",
      "         1.2838e-02,  2.0315e-01,  1.0746e-01, -1.0035e-01,  2.9137e-01,\n",
      "         3.9984e-01, -2.1050e-01, -2.2518e-01,  1.9023e-01, -4.1005e-02,\n",
      "        -2.4152e-01,  2.1422e-01,  9.6127e-02,  1.0931e-01, -9.4702e-02,\n",
      "        -3.5658e-01,  5.8907e-02,  1.8471e-01, -2.6443e-01,  5.1727e-02,\n",
      "         2.9061e-01, -8.5308e-02,  1.0933e-01,  1.9855e-01, -4.1235e-01,\n",
      "        -5.5536e-02,  2.1129e-02,  3.3484e-01, -5.5568e-02,  1.4471e-01,\n",
      "         1.1083e-01,  3.2459e-01, -1.8080e-01, -6.3783e-02, -8.8547e-02,\n",
      "         4.2879e-02, -1.8923e-01,  3.3412e-01, -1.6721e-01, -7.0848e-03,\n",
      "        -1.1664e-01, -5.2105e-02,  4.4875e-01,  2.4390e-01, -4.7686e-01,\n",
      "         2.7294e-01, -3.6256e-01, -9.6240e-02, -1.1398e-01,  6.6873e-02,\n",
      "         3.8281e-01, -8.0555e-02,  2.8811e-01, -2.6555e-02, -2.0603e-01,\n",
      "         5.1703e-01,  2.8236e-01, -2.4107e-32,  7.9571e-02, -3.8445e-02,\n",
      "         2.0096e-01, -2.4470e-02, -1.8760e-01, -2.3799e-01, -3.4400e-01,\n",
      "         1.0577e-01, -2.4676e-01, -4.2805e-02, -2.4475e-01, -1.9346e-01,\n",
      "         6.0236e-02,  2.2635e-01,  4.3402e-01, -1.7134e-01,  2.8169e-01,\n",
      "         1.4994e-01, -2.6966e-01,  2.7219e-02,  2.6482e-01, -6.8935e-01,\n",
      "         1.5750e-01, -2.4509e-01,  3.8428e-02,  1.9379e-02, -2.2785e-01,\n",
      "         1.2852e-01, -2.0983e-02,  1.2935e-01, -1.2989e-01,  4.8246e-02,\n",
      "        -2.6965e-01,  7.1386e-02, -8.9172e-03,  1.5093e-01,  4.7508e-01,\n",
      "        -1.4235e-01,  2.5790e-01, -2.2247e-01, -1.4045e-02,  3.9517e-02,\n",
      "        -2.1618e-01,  1.4695e-01, -4.6235e-01,  6.4458e-02, -1.1351e-01,\n",
      "        -7.5222e-02, -2.0113e-01,  3.9394e-01,  1.4455e-01,  3.8308e-02,\n",
      "        -6.5470e-02,  8.9205e-02,  2.2493e-03,  3.7324e-01, -2.1926e-01,\n",
      "        -2.0817e-01, -1.8055e-01,  1.4682e-01,  3.2651e-01, -3.1390e-02,\n",
      "         1.5185e-01, -8.0331e-03, -1.8611e-01,  1.2326e-02,  1.5385e-01,\n",
      "         2.1178e-02,  1.0165e-02,  3.0082e-01,  1.7629e-01, -1.2759e-02,\n",
      "        -9.9904e-02,  1.8567e-01, -1.2147e-01,  1.5969e-02,  1.3764e-01,\n",
      "        -4.9176e-02,  3.2488e-01,  9.0939e-02, -1.9138e-02, -4.2563e-01,\n",
      "        -3.9896e-01, -3.7157e-01,  1.6218e-01, -3.1053e-01,  2.9485e-01,\n",
      "        -2.2683e-01,  8.6052e-02, -5.5564e-01,  5.5332e-02, -3.3945e-02,\n",
      "         1.5930e-01, -1.4638e-01,  7.4580e-02,  2.1085e-32, -2.4106e-01,\n",
      "        -2.1405e-01, -3.0367e-01,  3.5056e-01,  2.8447e-01, -8.0113e-02,\n",
      "        -1.6110e-01,  1.3795e-01, -7.7149e-02,  4.8778e-01, -2.4114e-01,\n",
      "         1.5461e-01,  5.6559e-02,  4.7077e-02,  4.5989e-01, -2.0106e-01,\n",
      "         9.7115e-02, -1.2931e-01, -2.8787e-01,  1.1368e-01,  1.3541e-02,\n",
      "         4.8483e-02,  7.8761e-02, -3.6343e-01, -1.2479e-01,  1.2916e+00,\n",
      "        -1.1554e-01, -5.8212e-01, -1.7763e-01,  8.8866e-02,  1.1633e-01,\n",
      "        -1.6892e-01, -2.7695e-01, -2.7416e-01, -1.2398e-01,  3.6951e-02,\n",
      "         8.9207e-02, -6.6650e-02,  8.6561e-02,  1.8755e-01,  3.2505e-01,\n",
      "        -2.5603e-01, -2.1438e-01,  2.3641e-01, -4.1394e-03,  1.2410e-01,\n",
      "         3.5997e-01,  2.3648e-01, -2.3733e-01,  4.1122e-01,  9.6834e-02,\n",
      "        -1.1429e-01,  1.5954e-01,  1.4508e-01,  2.8479e-02,  1.5984e-01,\n",
      "        -5.3671e-02, -2.7635e-02,  1.6745e-01, -2.4418e-02,  6.7396e-04,\n",
      "         1.7007e-01, -3.2438e-01,  4.0043e-01,  2.5087e-01, -1.0752e-01,\n",
      "         1.3238e-01, -2.5911e-01, -5.8348e-02,  2.0865e-01, -2.2948e-01,\n",
      "         3.4534e-01, -3.1102e-01, -1.3470e-01, -7.0368e-02,  1.9910e-01,\n",
      "        -4.6402e-02,  2.9579e-01, -7.1953e-02, -2.6825e-01, -2.9969e-02,\n",
      "         2.3899e-01,  8.4630e-02,  5.3306e-02, -7.7880e-02,  1.3458e-01,\n",
      "         1.1872e-01,  2.1284e-01, -3.4688e-02,  1.7276e-01,  2.3269e-01,\n",
      "         4.6671e-01,  2.9103e-01,  3.8216e-01, -8.8343e-02, -9.5530e-08,\n",
      "        -1.3666e-01,  1.2520e-01, -8.4298e-02,  4.2311e-01,  9.3380e-02,\n",
      "        -7.2486e-02,  1.3641e-02,  3.7524e-01,  1.4411e-02,  1.0115e-01,\n",
      "         1.3496e-01, -2.0095e-01, -1.7724e-01, -1.4749e-02,  2.1624e-01,\n",
      "        -1.4222e-01,  1.2272e-01, -9.1905e-02, -9.0611e-02, -3.8412e-01,\n",
      "        -2.1590e-01, -1.0474e-01, -2.7086e-02,  9.1140e-03, -1.4780e-01,\n",
      "         1.2375e-01,  2.0462e-01,  2.4716e-01,  7.9220e-02, -5.8928e-02,\n",
      "         1.0810e-01,  2.1979e-01, -1.2139e-01,  3.2702e-02,  4.9911e-01,\n",
      "         1.8806e-01, -2.0690e-01, -2.0683e-01, -1.1906e-01,  1.9176e-02,\n",
      "        -2.6688e-01, -1.6895e-01,  5.8030e-02,  8.7635e-02, -2.8553e-02,\n",
      "         2.4369e-01, -1.8667e-01,  1.0287e-01,  7.1426e-02, -4.5617e-01,\n",
      "        -2.3231e-01, -1.8397e-01, -1.0922e-01, -4.9658e-02,  3.5070e-01,\n",
      "         2.5700e-01,  1.4587e-01, -1.9728e-02,  2.6029e-01, -1.4240e-01,\n",
      "         1.4145e-01, -3.9123e-01, -3.8495e-01,  1.4519e-01])\n",
      "\n",
      "Embedding for sentence 2: tensor([-6.4042e-01,  8.7785e-02,  2.9530e-02, -1.0334e-01, -4.7371e-02,\n",
      "         2.7491e-01,  3.6658e-01, -2.0420e-01, -2.8210e-01,  7.7412e-02,\n",
      "         7.2631e-02, -1.5417e-01,  3.6793e-02, -8.2736e-02,  2.9091e-01,\n",
      "        -1.2266e-01,  2.5182e-01,  1.1847e-01,  1.9822e-01, -4.8055e-01,\n",
      "        -3.0223e-01, -2.9898e-02,  1.1898e-02,  1.2331e-01, -2.4124e-01,\n",
      "         9.7400e-03, -2.5747e-01, -9.2628e-03, -4.9919e-02, -2.5344e-01,\n",
      "         5.9951e-02,  3.6450e-01, -5.2490e-02,  1.2256e-01, -4.7130e-01,\n",
      "        -1.6625e-02, -1.0687e-01, -1.4890e-01, -1.3995e-02, -2.0289e-01,\n",
      "        -1.4524e-01, -3.5394e-02, -3.0409e-01,  2.3126e-01, -6.4025e-02,\n",
      "         7.5216e-02, -1.7529e-01,  9.6175e-02, -1.0839e-01,  1.5974e-01,\n",
      "         8.4988e-02,  4.6748e-03,  1.7472e-01, -4.2416e-02, -4.3777e-04,\n",
      "         1.1387e-01,  2.4831e-01,  2.3933e-02,  7.0574e-02, -2.3821e-01,\n",
      "        -4.1467e-03, -2.9244e-01, -2.0298e+00,  4.2335e-02, -1.1961e-01,\n",
      "        -2.9309e-01, -9.5555e-02,  2.9810e-01,  5.6020e-02, -6.0421e-01,\n",
      "        -5.4390e-04, -5.6268e-02,  3.2942e-01, -1.6070e-01,  1.1637e-01,\n",
      "         2.6694e-01, -5.1833e-02, -2.6672e-01,  8.4948e-02,  2.7930e-01,\n",
      "        -3.9921e-01, -4.1273e-02, -6.1521e-02, -9.2117e-02,  8.7826e-02,\n",
      "        -1.6884e-01,  1.3955e-01, -2.0905e-01,  2.2191e-01, -1.6457e-01,\n",
      "        -2.3264e-02, -4.0862e-02,  2.1324e-01,  1.4952e-01, -6.3268e-01,\n",
      "         1.1738e-02, -1.5434e-01,  1.0814e-01, -1.4297e-01, -5.0013e-02,\n",
      "         2.2956e-01,  3.3807e-01,  9.3663e-02, -1.6047e-01, -9.0134e-02,\n",
      "        -4.1431e-01, -2.4825e-01,  4.2156e-01,  6.9614e-02, -7.8745e-02,\n",
      "        -2.9050e-01,  2.1023e-03,  2.4575e-01, -9.6150e-02, -2.2063e-01,\n",
      "         1.2442e-01, -3.5445e-01,  7.6520e-02, -7.3351e-02,  1.8976e-01,\n",
      "         7.2526e-01,  3.0412e-01,  1.8970e-01, -2.9754e-02, -6.3952e-02,\n",
      "         3.6446e-02,  4.9208e-01, -1.4572e-32,  1.7821e-01,  2.6633e-01,\n",
      "         2.1796e-01, -9.9250e-02,  2.0210e-01, -1.3283e-01,  2.8653e-01,\n",
      "         1.9184e-01, -2.8394e-01,  4.4891e-02, -1.3805e-01,  3.0874e-01,\n",
      "        -7.8649e-02,  4.9313e-01,  1.6272e-01, -1.0381e-01, -1.6645e-01,\n",
      "         4.7786e-02,  6.5252e-01, -5.1658e-02, -2.2984e-02,  3.0564e-02,\n",
      "         9.8654e-02, -3.8742e-02,  1.7347e-01,  4.1125e-01, -1.9974e-01,\n",
      "         4.0039e-01,  4.4100e-01,  5.8797e-02,  1.3425e-01, -5.1075e-02,\n",
      "        -6.6030e-02, -2.5662e-01, -3.0497e-01,  1.9942e-02, -7.4187e-02,\n",
      "        -2.3274e-01,  5.8500e-03, -1.8023e-01, -1.9670e-01, -3.0757e-01,\n",
      "         8.9856e-02,  1.3000e-01, -2.0720e-01, -1.2799e-01,  8.5247e-02,\n",
      "        -5.3162e-01,  4.7773e-02,  1.6420e-01, -3.1214e-01, -1.8962e-01,\n",
      "         2.3451e-01,  9.7289e-02, -1.8263e-01, -2.0332e-01, -2.0721e-01,\n",
      "        -1.4427e-01, -5.0148e-02, -1.2575e-01,  1.5063e-01, -2.7067e-01,\n",
      "         3.4084e-01, -1.3121e-01, -3.4714e-01, -1.5399e-01,  3.2283e-01,\n",
      "        -1.1031e-01,  3.6486e-02,  6.6175e-01,  1.8779e-01,  4.6696e-01,\n",
      "         2.2839e-01,  2.7075e-01, -1.6560e-01,  2.9014e-03, -1.8392e-01,\n",
      "         2.6325e-01,  2.8611e-01, -3.4781e-01,  1.6466e-01, -4.4500e-01,\n",
      "        -3.5282e-01,  2.5819e-01,  5.1208e-02,  2.6541e-01,  1.5363e-01,\n",
      "        -2.4029e-01,  3.6266e-02, -7.8635e-02,  1.7927e-01,  1.1878e-01,\n",
      "         2.0842e-01, -8.0734e-02,  2.9364e-01,  9.6146e-33,  1.1788e-01,\n",
      "        -3.1280e-01, -4.9670e-01,  2.5025e-01,  3.5624e-02,  9.9895e-02,\n",
      "        -2.0265e-01,  2.6027e-02,  7.3416e-02,  2.8151e-01, -4.6573e-01,\n",
      "         5.3351e-01,  4.0644e-02,  1.4635e-01, -3.0595e-02, -2.8344e-01,\n",
      "         9.5210e-02, -4.1212e-02, -1.6603e-01,  3.3427e-03,  1.3969e-01,\n",
      "         2.9298e-01, -5.1886e-01, -2.8220e-01, -6.3341e-02,  8.8400e-01,\n",
      "         2.0915e-01, -2.2199e-01, -2.8540e-01, -7.9074e-04, -6.5481e-03,\n",
      "        -1.8666e-01, -1.6036e-01, -1.8043e-01,  5.9740e-02, -1.2516e-01,\n",
      "         2.6921e-02, -9.0424e-02, -2.0916e-01,  4.3286e-01, -4.6949e-02,\n",
      "        -2.8302e-01,  1.6881e-02,  2.9404e-01, -1.1528e-01,  4.7193e-02,\n",
      "         2.9775e-01,  2.5883e-01,  1.7024e-02,  2.5310e-02,  7.2452e-02,\n",
      "        -1.3503e-01,  1.2384e-02, -4.3834e-01, -1.7725e-01,  4.7688e-01,\n",
      "        -1.4843e-01, -3.2145e-01,  2.3137e-01,  1.2478e-02, -1.2900e-01,\n",
      "         1.2101e-01, -1.8946e-01,  5.3814e-01, -1.0051e-01, -1.3379e-01,\n",
      "         2.0161e-01, -2.7759e-01,  7.5332e-02,  1.9091e-03,  2.4629e-01,\n",
      "         2.0687e-01,  3.9289e-02, -2.6200e-01, -4.1902e-01, -6.9389e-02,\n",
      "         4.6538e-02, -4.6435e-02,  1.0844e-02,  7.2780e-02,  3.9404e-02,\n",
      "         8.2464e-02,  4.5288e-01, -6.5774e-02, -1.1323e-01, -4.3023e-02,\n",
      "        -2.6080e-01,  2.0660e-01, -1.6464e-01, -1.8402e-01, -4.9261e-02,\n",
      "         2.3385e-01,  1.7462e-01,  5.3323e-01,  1.0210e-01, -9.6700e-08,\n",
      "         3.5030e-01, -4.4629e-02,  4.4828e-02,  4.3235e-01,  2.2266e-01,\n",
      "         5.7708e-03, -2.0908e-01,  2.7614e-01, -2.4418e-01,  3.4130e-02,\n",
      "         1.4316e-01,  1.2515e-01, -1.7802e-01,  8.7491e-02,  4.0399e-01,\n",
      "         1.5475e-01,  5.3150e-01,  1.6258e-01, -6.6805e-02, -3.3762e-01,\n",
      "        -4.4257e-02,  5.0040e-02,  1.9980e-01, -3.2468e-01, -1.7334e-02,\n",
      "        -6.0562e-02,  1.0144e-01,  3.0972e-01,  3.4034e-01, -3.5598e-01,\n",
      "         3.7145e-01,  7.5940e-02,  3.0646e-01,  1.3847e-01,  8.4675e-01,\n",
      "         1.8036e-01, -7.3597e-02, -4.6688e-02,  8.0192e-02,  1.5676e-01,\n",
      "        -2.6719e-01, -4.0938e-01, -7.6397e-02,  1.5496e-01, -6.4415e-01,\n",
      "         2.3005e-01, -2.0869e-01, -7.6561e-02,  2.2548e-02, -2.7399e-01,\n",
      "        -7.5984e-02, -1.8259e-01, -1.1424e-01,  1.3578e-01,  4.9054e-01,\n",
      "        -4.3057e-02, -3.3113e-02, -1.6514e-01,  2.1884e-01,  2.4537e-01,\n",
      "         4.7605e-01, -2.4025e-01, -4.4046e-01,  5.8492e-02])\n",
      "\n",
      "Embedding for sentence 3: tensor([-5.5165e-01,  9.9192e-02,  1.0532e-01,  1.6842e-01, -1.7392e-01,\n",
      "        -6.4917e-02, -9.2957e-02,  1.3019e-01,  9.3348e-02, -2.5461e-01,\n",
      "         5.8768e-02,  3.3547e-01, -8.5629e-02,  2.0683e-02,  2.0508e-01,\n",
      "        -1.8285e-01,  2.1118e-01, -4.7194e-01,  1.4798e-01, -8.5724e-02,\n",
      "        -1.4490e-01, -9.9034e-02,  4.5316e-02, -5.4787e-02, -4.6091e-02,\n",
      "        -1.4944e-01, -5.7407e-01, -3.8291e-01,  1.6977e-01,  6.5790e-02,\n",
      "         2.0375e-01,  4.0218e-01, -2.0369e-01,  1.4160e-01, -2.2517e-01,\n",
      "        -1.1384e-01, -5.8783e-02,  2.4038e-01, -1.3917e-01,  4.7918e-02,\n",
      "        -1.9155e-01,  1.1286e-01, -1.5443e-02,  1.0037e-01,  1.6447e-02,\n",
      "        -1.5987e-01,  2.2319e-01,  1.3544e-01, -7.4138e-02,  2.7134e-02,\n",
      "        -2.0653e-01,  9.6083e-02,  2.9126e-01, -6.1128e-02,  6.4133e-02,\n",
      "        -1.8372e-01,  2.3748e-02, -2.1253e-03, -2.4069e-02,  7.4211e-02,\n",
      "        -3.1089e-01, -2.5090e-01, -7.8205e-02, -1.1982e-01, -1.0275e-01,\n",
      "        -1.3317e-01,  1.1168e-01,  2.3997e-01,  3.4914e-01, -4.4243e-01,\n",
      "        -1.2472e-02,  5.2684e-02,  8.7065e-02,  1.3450e-01,  2.6584e-01,\n",
      "         1.6832e-01,  3.2723e-01, -1.2163e-01, -7.8142e-02,  2.4190e-02,\n",
      "        -4.1207e-01, -6.7998e-02,  8.6977e-02, -1.8267e-02, -1.9575e-02,\n",
      "         1.0502e-01, -4.7795e-04, -1.4483e-01,  3.5059e-02, -2.1600e-01,\n",
      "         1.6759e-01,  1.3208e-01,  8.5097e-02,  1.7720e-01, -2.1276e-01,\n",
      "        -9.9508e-02,  5.7012e-02,  2.8466e-02, -2.3462e-01, -4.0526e-02,\n",
      "         5.8523e-02,  4.2348e-01,  7.7606e-02,  7.7682e-02,  1.3502e-01,\n",
      "        -2.3672e-01, -6.1295e-02,  1.7285e-01, -6.8346e-02, -2.9550e-04,\n",
      "        -3.7714e-01,  7.1172e-02, -1.8812e-02, -7.1506e-02, -5.5936e-01,\n",
      "        -4.3157e-03, -3.3800e-01, -1.0441e-01,  6.8165e-02,  2.2047e-01,\n",
      "         3.7628e-01,  4.0077e-02,  1.1968e-01, -1.0964e-01, -4.5491e-01,\n",
      "        -2.8859e-02,  1.2177e-01, -8.3758e-33,  8.0820e-02,  1.9032e-01,\n",
      "        -1.0421e-01,  4.8386e-02, -1.7773e-01, -2.3905e-01,  9.1650e-02,\n",
      "         1.6752e-01,  4.1417e-02, -1.1040e-01,  2.2245e-02,  1.8657e-01,\n",
      "        -1.5589e-01,  1.7206e-01,  1.0362e-01,  8.4093e-02, -3.2356e-01,\n",
      "         2.2512e-02,  3.0875e-01,  1.1442e-01,  6.4361e-02, -5.7671e-02,\n",
      "         2.4252e-01, -6.0784e-02,  2.4250e-01,  2.2308e-01, -1.3759e-01,\n",
      "         5.2228e-01,  3.8888e-01, -4.2581e-02,  2.5891e-01, -4.0258e-01,\n",
      "         9.5627e-02, -2.0399e-01,  8.5080e-03,  1.7140e-01,  2.8616e-01,\n",
      "        -1.6535e-01,  1.6998e-01, -2.2569e-01, -1.8061e-01,  9.2877e-02,\n",
      "         1.0531e-01,  4.4814e-03,  4.9784e-03,  7.5759e-02,  3.8933e-01,\n",
      "        -1.5624e-01, -2.9912e-01,  2.3284e-01, -1.2447e-01, -4.5858e-02,\n",
      "        -6.4502e-02, -4.6326e-02, -7.9522e-02, -5.1746e-01,  1.6550e-01,\n",
      "        -4.5801e-03,  2.0648e-01, -1.3484e-01, -9.4011e-02, -2.0924e-01,\n",
      "         1.9209e-01, -2.7815e-01, -2.9680e-01, -1.0119e-02,  1.7729e-01,\n",
      "         2.7610e-01, -3.2745e-01,  3.9745e-01,  3.0766e-01,  2.7309e-01,\n",
      "        -1.5799e-01, -4.4315e-01,  1.0663e-01, -1.6851e-01,  5.5688e-02,\n",
      "         1.8230e-01,  2.1371e-01, -1.8043e-01,  3.7753e-02, -2.4205e-01,\n",
      "        -6.0273e-02,  4.4537e-01, -7.3513e-02,  4.8238e-01,  1.9752e-01,\n",
      "         1.1030e-01,  8.5282e-02,  1.6972e-01, -3.7203e-02,  8.7854e-02,\n",
      "         4.0759e-01, -9.9566e-02,  1.1016e-01,  3.6538e-33,  2.7448e-02,\n",
      "        -3.1450e-01, -2.9833e-01,  1.2706e-01, -1.2891e-01, -1.0543e-01,\n",
      "         5.6385e-02,  2.1432e-01,  2.3732e-01, -1.2103e-01, -1.6790e-01,\n",
      "         1.2845e-01,  1.8354e-01,  1.0307e-01,  1.5684e-01, -1.7479e-01,\n",
      "         1.3935e-01,  4.5137e-02,  2.0486e-01, -2.5833e-01,  4.6140e-02,\n",
      "         4.4536e-01, -4.2305e-01, -2.4285e-02, -2.4346e-01,  4.0362e-02,\n",
      "        -5.5824e-02, -2.0536e-01, -4.7685e-01, -2.8925e-01,  1.7482e-01,\n",
      "        -3.7044e-01, -3.9306e-01,  1.3571e-01, -9.9856e-02,  5.5551e-02,\n",
      "        -3.2898e-01,  2.6318e-01,  1.2644e-01,  7.8980e-02, -2.9505e-01,\n",
      "         1.5091e-01, -8.2778e-02,  1.5334e-01, -1.8567e-02, -1.4895e-01,\n",
      "        -6.3031e-02,  5.1012e-02,  4.9114e-01,  1.8853e-01,  6.3200e-03,\n",
      "        -1.0891e-01,  8.0305e-02, -5.4773e-01, -3.8070e-01,  2.4018e-01,\n",
      "         8.3073e-02,  2.8519e-02,  2.1061e-01, -1.3316e-01, -1.7461e-01,\n",
      "        -2.2699e-01, -2.3935e-01, -9.5293e-02, -1.4775e-01,  9.1046e-02,\n",
      "         2.6084e-01,  3.6705e-02, -6.1435e-02,  1.2969e-01,  3.2654e-01,\n",
      "         2.0329e-01,  3.1171e-02,  6.9782e-02, -2.7661e-01,  4.2321e-01,\n",
      "         6.9373e-02,  6.3984e-02,  5.2875e-02, -2.9590e-01, -6.3418e-02,\n",
      "        -4.4045e-02,  1.5958e-01,  6.5875e-02,  6.6991e-02, -8.3760e-02,\n",
      "        -1.5487e-01, -3.2048e-01, -1.4365e-01, -1.7490e-01, -1.1532e-01,\n",
      "        -2.4014e-01,  9.1985e-02,  3.4495e-01,  8.4234e-02, -9.7948e-08,\n",
      "         3.5257e-01,  3.2626e-02, -4.7406e-02,  1.4550e-01,  2.4055e-01,\n",
      "         1.6362e-01, -2.0801e-02,  1.9641e-01,  7.1534e-02, -8.3343e-02,\n",
      "         1.9210e-01,  2.3226e-01, -2.0084e-01, -2.8412e-01,  3.1822e-01,\n",
      "        -4.0799e-02,  8.9095e-02, -1.7528e-01, -1.4888e-01, -2.2411e-01,\n",
      "         5.0100e-02, -6.6001e-02,  3.2636e-01, -2.0468e-01, -2.0975e-01,\n",
      "         2.1212e-01,  2.9233e-01, -1.0714e-01,  1.5776e-01, -3.1571e-01,\n",
      "         1.2140e-01, -2.6739e-01,  4.1849e-01,  7.5529e-02,  6.1558e-01,\n",
      "         1.2401e-01, -3.1703e-01,  2.3730e-02, -7.5047e-02,  1.1022e-01,\n",
      "         1.4867e-01, -2.1353e-01, -3.6270e-02,  4.6314e-02, -4.6096e-01,\n",
      "         1.9784e-01, -1.8590e-01,  1.5668e-01,  1.6090e-01,  1.5452e-01,\n",
      "        -4.5997e-02, -2.5408e-01, -2.1586e-01,  1.0296e-01,  6.3330e-03,\n",
      "        -1.5103e-01,  9.1846e-02, -3.8209e-01,  3.4197e-01, -4.1576e-02,\n",
      "         2.2465e-01,  5.3639e-02, -1.7016e-01,  3.2707e-01])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showcase embeddings\n",
    "for i, emb in enumerate(sentence_embeddings):\n",
    "    print(f\"Embedding for sentence {i+1}: {emb}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model Selection:**\n",
    "I chose the sentence-transformers/all-MiniLM-L6-v2 model becuase its lightweight and reliable. Since this is the first step of a multistep take home project that I'm running on my computer a lightweight model is the ideal option for balancing speed and accuracy. Additionally, this model is already fine-tuned for sentence-level tasks like semantic similarity, which reduces the need for additional fine-tuning. Some other pre trained base encoder models I could of used are: BERT, Distil-BERT, ALBERT, RoBERTa, XLNET, etc...\n",
    "\n",
    "2. **Pooling Strategy:**\n",
    "I applied mean pooling across the token dimension of the last hidden state to create fixed-length sentence embeddings since the request was to return \"fix length embeddings\". Mean pooling provides a robust rebresentation of the whole embedding. My other option would have been to just grab the first x embs for each sentance but this would not have captured the whole meaning of the sentance as well.\n",
    "\n",
    "3. **Padding and Truncation:**\n",
    "I set both padding=True and truncation=True when tokenizing the input sentences. Padding ensures that all sentences in the batch have the same length, allowing for batch processing in parallel. Truncation cuts off sentance that are longer than the modelâ€™s maximum input length to avoid errors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multi-Task Learning Expansion\n",
    "Expand the sentence transformer model architecture to handle a multi-task learning setting.\n",
    "- Task A: Sentence Classification\n",
    "    - Implement a task-specific head for classifying sentences into predefined classes\n",
    "    - Classify sentences into predefined classes (you can make these up).\n",
    "- Task B: [Choose an Additional NLP Task]\n",
    "    - Implement a second task-specific head for a different NLP task, such as Named Entity Recognition (NER) or Sentiment Analysis (you can make the labels up).\n",
    "- Discuss the changes made to the architecture to support multi-task learning.\n",
    "Note that itâ€™s not required to actually train the multi-task learning model or implement a training\n",
    "loop. The focus is on implementing a forward pass that can accept an input sentence and output\n",
    "predictions for each task that you define."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Classification and Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model, this is staying the same from step 1\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multi-Task Model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_task_a, num_classes_task_b):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.transformer = base_model\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Task A: Sentence Classification head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes_task_a)\n",
    "        )\n",
    "        \n",
    "        # Task B: Sentiment Analysis head\n",
    "        self.sentiment_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes_task_b)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass through transformer model\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        cls_token_rep = last_hidden_state[:, 0, :] \n",
    "        \n",
    "        # Task A: Sentence classification \n",
    "        task_a_logits = self.classification_head(cls_token_rep)\n",
    "        \n",
    "        # Task B: Sentiment Analysis\n",
    "        task_b_logits = self.sentiment_head(cls_token_rep)\n",
    "        \n",
    "        return task_a_logits, task_b_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (Sanity check not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task A (Sentence Classification) Probabilities:\n",
      "tensor([[0.4653, 0.5347],\n",
      "        [0.4676, 0.5324],\n",
      "        [0.4469, 0.5531]])\n",
      "Task B (Sentiment Analysis) Probabilities:\n",
      "tensor([[0.3596, 0.3431, 0.2974],\n",
      "        [0.3577, 0.3409, 0.3014],\n",
      "        [0.3522, 0.3561, 0.2917]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the multi-task model\n",
    "num_classes_task_a = 2  # like: happy, sad \n",
    "num_classes_task_b = 3  # like: positive, neutral, negative\n",
    "model = MultiTaskModel(base_model, num_classes_task_a, num_classes_task_b)\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"The service was okay, not great.\",\n",
    "    \"I'm disappointed with the quality of the item.\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "encoding = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    task_a_logits, task_b_logits = model(input_ids, attention_mask)\n",
    "    \n",
    "# Convert logits to probabilities and print\n",
    "task_a_probs = torch.softmax(task_a_logits, dim=-1)\n",
    "task_b_probs = torch.softmax(task_b_logits, dim=-1)\n",
    "\n",
    "print(\"\\nTask A (Sentence Classification) Probabilities:\")\n",
    "print(task_a_probs)\n",
    "print(\"Task B (Sentiment Analysis) Probabilities:\")\n",
    "print(task_b_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sentence Classification and Sentiment Analysis heads:**\n",
    "For simplicity's sake, I implemented both heads the same way. I tried to balance model complexity, computational efficiency, and generalization. Here is my reasoning for the implementation of the head layers:\n",
    "    - nn.sequential, so I don't need to code each step in the forward \n",
    "    - The initial linear layer reduces the high-dimensional output from the transformer to a more manageable size (128 units), which helps focus on the relevant features.\n",
    "    - ReLU introduces non-linearity, allowing the model to learn more complex patterns.\n",
    "    - Dropout helps prevent overfitting by making the model robust to variations and noise in the data.\n",
    "    - The second linear layer maps the reduced representation to the final output space, providing class logits for the classification task.  \n",
    "2. **Hidden layer:** I added a hidden layer that is shared by both tasks. These layers capture common features, improve generalizations, and reduce overfitting.  \n",
    "3. **The forward method:**\n",
    "I added the Forward pass, which runs both sentence classification and sentiment analysis for every input sentence. If these tasks are not needed each time, adding an ```if else``` statement and adding a task_id as an input would be a great way to save on computation.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Discussion Questions\n",
    "1. Consider the scenario of training the multi-task sentence transformer that you implemented in Task 2. Specifically, discuss how you would decide which portions of the network to train and which parts to keep frozen.\n",
    "For example,\n",
    "    - When would it make sense to freeze the transformer backbone and only train the task specific layers?\n",
    "    - When would it make sense to freeze one head while training the other?\n",
    "2. Discuss how you would decide when to implement a multi-task model like the one in this\n",
    "assignment and when it would make more sense to use two completely separate models\n",
    "for each task.\n",
    "3. When training the multi-task model, assume that Task A has abundant data, while Task B has limited data. Explain how you would handle this imbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers \n",
    "1. If I were to train the multi-task sentence trasnformer that I implemented in task 2, I would freeze the transformer backbone and only train the specific task layers:\n",
    "    - if the data the transformer backbone was trained on is similar to my task data (e.g. the backbone model I used in this example was trained on sentance level task so I would probably freeze it during training)\n",
    "    - if I had limited task layer data (so that the model does not overfit the data)\n",
    "    - to save on computational resources, any pretrained transformer backbone does not *need* to be fine tuned, so if I have limited resources, I would focus on the task layers \n",
    "\n",
    "    I would freeze one head while training the other when: \n",
    "    - I've achieved my goal performance for one head but want to continue training and fine-tuning the other task head \n",
    "    - I need to transfer knowledge from one head to another (I would freeze the head that is performing well and continue to train the other head)\n",
    "\n",
    "2. In general, I would implement a multi-task model like this one when the tasks are related and/or I don't have enough data to implement individual models. The shared embedding architecture of both tasks and likeness in objective make them great candidates for multi-task modeling. I've observed that often, however, when you have enough data for each task, individual models perform better. Having two seperate models does increase complexity so one should weigh that. \n",
    "3. If Task A has abundant data and Task B has limited data, I would try out a few methods to handle this task imbalance and use whichever works best: \n",
    "    - undersample the Task A data\n",
    "    - up-sample Task B data (could try SMOTE or other data augmentation techniques)\n",
    "    - use task-specific loss weighting, giving more importance to Task B during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
